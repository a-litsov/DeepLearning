{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.nasnet import NASNetLarge\n",
    "from tensorflow.keras import models\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 images belonging to 6 classes.\n",
      "Found 3000 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH = 128\n",
    "image_size = 150\n",
    "def generator():\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    train_data = datagen.flow_from_directory('/kaggle/input/intel-image-classification/seg_train/seg_train/',\n",
    "                                        target_size=(image_size, image_size),\n",
    "                                        batch_size=BATCH,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=True)\n",
    "\n",
    "    test_data = datagen.flow_from_directory('/kaggle/input/intel-image-classification/seg_test/seg_test/',\n",
    "                                        target_size=(image_size, image_size),\n",
    "                                        batch_size=BATCH,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=True)\n",
    "\n",
    "    return train_data, test_data\n",
    "train_data, test_data = generator()\n",
    "train_images, train_labels = train_data.next()\n",
    "test_images, test_labels = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "(56, 150, 150, 3)\n",
      "110\n",
      "(82, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))\n",
    "print(test_data[23][0].shape)\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data[109][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 150, 150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'buildings': 0,\n",
       " 'forest': 1,\n",
       " 'glacier': 2,\n",
       " 'mountain': 3,\n",
       " 'sea': 4,\n",
       " 'street': 5}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Sat Dec 14 22:16:23 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pprint\n",
    "\n",
    "sp = subprocess.Popen(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "out_str = sp.communicate()\n",
    "out_list = str(out_str[0]).split('\\\\n')\n",
    "\n",
    "out_dict = {}\n",
    "\n",
    "for item in out_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент 1\n",
    "Использование структуры глубокой модели, построенной для решения исходной «Задачи А», с целью обучения аналогичной модели для решения «Задачи В»\n",
    "* Предполагается, что модель, построенная для решения исходной задачи, обучается на данных, подготовленных для\n",
    "решения целевой задачи\n",
    "* При этом веса модели инициализируются случайным образом\n",
    "* Эксперимент реализует перенос знаний для родственных доменов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем VGG16 - модель для решения задачи ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 6)                 24582     \n",
      "=================================================================\n",
      "Total params: 65,079,110\n",
      "Trainable params: 65,079,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model = VGG16(include_top=True, weights=None, input_shape=(image_size, image_size, 3), classes=6)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 110 steps, validate for 24 steps\n",
      "Epoch 1/20\n",
      "110/110 [==============================] - 69s 626ms/step - loss: 1475.8409 - accuracy: 0.1726 - val_loss: 1.7865 - val_accuracy: 0.2107\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 53s 483ms/step - loss: 2.9337 - accuracy: 0.1756 - val_loss: 1.7908 - val_accuracy: 0.1750\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 1.7945 - accuracy: 0.1781 - val_loss: 1.7901 - val_accuracy: 0.1843\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 1.9892 - accuracy: 0.1749 - val_loss: 1.7905 - val_accuracy: 0.1750\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 53s 485ms/step - loss: 1.8491 - accuracy: 0.1804 - val_loss: 2.6758 - val_accuracy: 0.3040\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 1.8588 - accuracy: 0.1794 - val_loss: 1.8168 - val_accuracy: 0.1457\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 53s 485ms/step - loss: 2.1758 - accuracy: 0.1852 - val_loss: 1.7893 - val_accuracy: 0.1750\n",
      "Epoch 8/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 2.1063 - accuracy: 0.1855 - val_loss: 1.7829 - val_accuracy: 0.1813\n",
      "Epoch 9/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 1.8231 - accuracy: 0.1812 - val_loss: 1.7835 - val_accuracy: 0.1953\n",
      "Epoch 10/20\n",
      "110/110 [==============================] - 53s 485ms/step - loss: 3.5281 - accuracy: 0.2168 - val_loss: 1.5932 - val_accuracy: 0.3087\n",
      "Epoch 11/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 1.9523 - accuracy: 0.2074 - val_loss: 1.6178 - val_accuracy: 0.3347\n",
      "Epoch 12/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 1.7881 - accuracy: 0.2836 - val_loss: 2.8871 - val_accuracy: 0.2210\n",
      "Epoch 13/20\n",
      "110/110 [==============================] - 53s 485ms/step - loss: 1.3877 - accuracy: 0.4600 - val_loss: 1.2567 - val_accuracy: 0.5327\n",
      "Epoch 14/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 1.1123 - accuracy: 0.5566 - val_loss: 1.0149 - val_accuracy: 0.5840\n",
      "Epoch 15/20\n",
      "110/110 [==============================] - 53s 486ms/step - loss: 1.0927 - accuracy: 0.5998 - val_loss: 0.9236 - val_accuracy: 0.6433\n",
      "Epoch 16/20\n",
      "110/110 [==============================] - 53s 483ms/step - loss: 0.9924 - accuracy: 0.6253 - val_loss: 0.8470 - val_accuracy: 0.6637\n",
      "Epoch 17/20\n",
      "110/110 [==============================] - 53s 484ms/step - loss: 0.8867 - accuracy: 0.6625 - val_loss: 0.7994 - val_accuracy: 0.6903\n",
      "Epoch 18/20\n",
      "110/110 [==============================] - 54s 486ms/step - loss: 0.8780 - accuracy: 0.6862 - val_loss: 0.7010 - val_accuracy: 0.7220\n",
      "Epoch 19/20\n",
      "110/110 [==============================] - 53s 483ms/step - loss: 0.7573 - accuracy: 0.7157 - val_loss: 0.7030 - val_accuracy: 0.7303\n",
      "Epoch 20/20\n",
      "110/110 [==============================] - 53s 483ms/step - loss: 0.7288 - accuracy: 0.7320 - val_loss: 0.6992 - val_accuracy: 0.7287\n",
      "Time:  0:18:01.258895\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "vgg_model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "time_start = datetime.now()\n",
    "history = vgg_model.fit_generator(train_data, steps_per_epoch=len(train_data), shuffle=True, epochs=EPOCHS, validation_steps=len(test_data), validation_data=test_data)\n",
    "time = datetime.now() - time_start\n",
    "print('Time: ', time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент 2\n",
    "Использование модели, построенной для решения исходной «Задачи А», в качестве фиксированного\n",
    "метода извлечения признаков при построении модели, решающей «Задачу В»\n",
    "* Идея данного подхода состоит в том, чтобы удалить из глубокой модели классификатор (последние полностью\n",
    "связанные слои) и рассматривать начальную часть сети как метод выделения признаков\n",
    "* Взамен старого классификатора можно поместить новый классификатор (например, другой набор полностью\n",
    "связанных слоев или машину опорных векторов) и обучить его на признаках, построенных с использованием начальной\n",
    "части сети\n",
    "* Эксперимент реализует перенос признакового описания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем за основу сверточную нейронную сеть VGG16, обученную на наборе ImageNet (1,4 миллиона изобр, классифицированных на 1000 классов).\n",
    "\n",
    "Пропустим набор данных intel-image через предварительно обученную сверточную основу VGG16, запишем результат.\n",
    "\n",
    "Используем результат как входные данные для отдельного полносвязного классификатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 1s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(image_size, image_size, 3), classes=1000)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sample_count, vgg_model, generat):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count, 6))\n",
    "    i=0\n",
    "    for inputs_batch, labels_batch in generat:\n",
    "        #выделяем признаки из изображений\n",
    "        features_batch = vgg_model.predict(inputs_batch)\n",
    "        features[i * BATCH : (i + 1) * BATCH] = features_batch\n",
    "        labels[i * BATCH : (i + 1) * BATCH] = labels_batch\n",
    "        i = i + 1\n",
    "        if (i + 1) * BATCH >= sample_count:\n",
    "            return features, labels\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14034 images belonging to 6 classes.\n",
      "Found 3000 images belonging to 6 classes.\n",
      "Time:  0:00:51.969965\n"
     ]
    }
   ],
   "source": [
    "time_start = datetime.now()\n",
    "train_sample_count = 14034\n",
    "test_sample_count = 3000\n",
    "generat = generator()\n",
    "#выделяем признаки\n",
    "train_features, train_y = extract_features(train_sample_count, vgg_model, generat[0])\n",
    "test_features, test_y = extract_features(test_sample_count, vgg_model, generat[1])\n",
    "\n",
    "time = datetime.now() - time_start\n",
    "print('Time: ', time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14034, 4, 4, 512)\n",
      "(14034, 6)\n",
      "(3000, 4, 4, 512)\n",
      "(3000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(test_features.shape) \n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.reshape(train_features, (train_sample_count, 4*4*512))\n",
    "test_features = np.reshape(test_features, (test_sample_count, 4*4*512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14034, 8192)\n",
      "(14034, 6)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1000)              8193000   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 6006      \n",
      "=================================================================\n",
      "Total params: 8,199,006\n",
      "Trainable params: 8,199,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14034 samples, validate on 3000 samples\n",
      "Epoch 1/20\n",
      "14034/14034 [==============================] - 1s 102us/sample - loss: 1.3352 - accuracy: 0.7306 - val_loss: 0.4588 - val_accuracy: 0.8197\n",
      "Epoch 2/20\n",
      "14034/14034 [==============================] - 1s 71us/sample - loss: 0.4347 - accuracy: 0.8345 - val_loss: 0.3976 - val_accuracy: 0.8280\n",
      "Epoch 3/20\n",
      "14034/14034 [==============================] - 1s 71us/sample - loss: 0.3418 - accuracy: 0.8690 - val_loss: 0.5574 - val_accuracy: 0.7993\n",
      "Epoch 4/20\n",
      "14034/14034 [==============================] - 1s 73us/sample - loss: 0.2884 - accuracy: 0.8850 - val_loss: 0.3289 - val_accuracy: 0.8547\n",
      "Epoch 5/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.2412 - accuracy: 0.9082 - val_loss: 0.3388 - val_accuracy: 0.8780\n",
      "Epoch 6/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.2046 - accuracy: 0.9211 - val_loss: 0.4136 - val_accuracy: 0.8593\n",
      "Epoch 7/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.1728 - accuracy: 0.9337 - val_loss: 0.3651 - val_accuracy: 0.8763\n",
      "Epoch 8/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.1420 - accuracy: 0.9443 - val_loss: 0.3726 - val_accuracy: 0.8903\n",
      "Epoch 9/20\n",
      "14034/14034 [==============================] - 1s 71us/sample - loss: 0.1225 - accuracy: 0.9513 - val_loss: 0.4053 - val_accuracy: 0.8880\n",
      "Epoch 10/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.1038 - accuracy: 0.9578 - val_loss: 0.4335 - val_accuracy: 0.8690\n",
      "Epoch 11/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.0897 - accuracy: 0.9635 - val_loss: 0.4627 - val_accuracy: 0.8610\n",
      "Epoch 12/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.0797 - accuracy: 0.9664 - val_loss: 0.5146 - val_accuracy: 0.8540\n",
      "Epoch 13/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.0623 - accuracy: 0.9714 - val_loss: 0.5503 - val_accuracy: 0.8550\n",
      "Epoch 14/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.0585 - accuracy: 0.9745 - val_loss: 0.5218 - val_accuracy: 0.8700\n",
      "Epoch 15/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.0572 - accuracy: 0.9781 - val_loss: 0.5544 - val_accuracy: 0.8717\n",
      "Epoch 16/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.0586 - accuracy: 0.9788 - val_loss: 0.6948 - val_accuracy: 0.8567\n",
      "Epoch 17/20\n",
      "14034/14034 [==============================] - 1s 72us/sample - loss: 0.0470 - accuracy: 0.9809 - val_loss: 0.6279 - val_accuracy: 0.8713\n",
      "Epoch 18/20\n",
      "14034/14034 [==============================] - 1s 73us/sample - loss: 0.0406 - accuracy: 0.9829 - val_loss: 0.6900 - val_accuracy: 0.8593\n",
      "Epoch 19/20\n",
      "14034/14034 [==============================] - 1s 71us/sample - loss: 0.0313 - accuracy: 0.9845 - val_loss: 0.6785 - val_accuracy: 0.8660\n",
      "Epoch 20/20\n",
      "14034/14034 [==============================] - 1s 71us/sample - loss: 0.0398 - accuracy: 0.9829 - val_loss: 0.7552 - val_accuracy: 0.8647\n",
      "Time:  0:00:21.524430\n"
     ]
    }
   ],
   "source": [
    "#передадим полученные признаки на вход полносвязному классификатору\n",
    "EPOCHS = 20\n",
    "ACTIVATION='relu'\n",
    "KERNEL_INIT='he_normal'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(1000, activation=ACTIVATION, input_dim=4*4*512, kernel_initializer=KERNEL_INIT))\n",
    "\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "time_start = datetime.now()\n",
    "history = model.fit(train_features, train_y, epochs=EPOCHS, batch_size = 128, shuffle=True, validation_data=(test_features, test_y))\n",
    "time = datetime.now() - time_start\n",
    "print('Time: ', time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Эксперимент 3\n",
    "Тонкая настройка параметров модели, построенной для решения исходной «Задачи А», с целью решения\n",
    "«Задачи В»\n",
    "* Последние слои глубокой модели, соответствующие классификатору, который решает «Задачу А», заменяются\n",
    "новым классификатором (например, набором полностью связанных слоев с другим количеством выходов)\n",
    "* Полученная модель обучается как единая система \n",
    "* Эксперимент реализует перенос обучения на основе\n",
    "экземпляров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model = VGG16(include_top=False, weights='imagenet', input_shape=(image_size, image_size, 3), classes=1000)\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              8193000   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 6006      \n",
      "=================================================================\n",
      "Total params: 22,913,694\n",
      "Trainable params: 8,199,006\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Train for 110 steps, validate for 24 steps\n",
      "Epoch 1/20\n",
      "110/110 [==============================] - 31s 281ms/step - loss: 1.3981 - accuracy: 0.7326 - val_loss: 0.4030 - val_accuracy: 0.8543\n",
      "Epoch 2/20\n",
      "110/110 [==============================] - 30s 275ms/step - loss: 0.4732 - accuracy: 0.8303 - val_loss: 0.4898 - val_accuracy: 0.8183\n",
      "Epoch 3/20\n",
      "110/110 [==============================] - 29s 264ms/step - loss: 0.3595 - accuracy: 0.8638 - val_loss: 0.3441 - val_accuracy: 0.8687\n",
      "Epoch 4/20\n",
      "110/110 [==============================] - 29s 266ms/step - loss: 0.2896 - accuracy: 0.8906 - val_loss: 0.4947 - val_accuracy: 0.8190\n",
      "Epoch 5/20\n",
      "110/110 [==============================] - 31s 284ms/step - loss: 0.2511 - accuracy: 0.9046 - val_loss: 0.3434 - val_accuracy: 0.8727\n",
      "Epoch 6/20\n",
      "110/110 [==============================] - 30s 273ms/step - loss: 0.2051 - accuracy: 0.9213 - val_loss: 0.3969 - val_accuracy: 0.8570\n",
      "Epoch 7/20\n",
      "110/110 [==============================] - 30s 277ms/step - loss: 0.1730 - accuracy: 0.9356 - val_loss: 0.4420 - val_accuracy: 0.8653\n",
      "Epoch 8/20\n",
      "110/110 [==============================] - 30s 275ms/step - loss: 0.1526 - accuracy: 0.9446 - val_loss: 0.3584 - val_accuracy: 0.8877\n",
      "Epoch 9/20\n",
      "110/110 [==============================] - 30s 275ms/step - loss: 0.1259 - accuracy: 0.9509 - val_loss: 0.4299 - val_accuracy: 0.8657\n",
      "Epoch 10/20\n",
      "110/110 [==============================] - 30s 274ms/step - loss: 0.1094 - accuracy: 0.9604 - val_loss: 0.5338 - val_accuracy: 0.8487\n",
      "Epoch 11/20\n",
      "110/110 [==============================] - 30s 270ms/step - loss: 0.0932 - accuracy: 0.9668 - val_loss: 0.6369 - val_accuracy: 0.8427\n",
      "Epoch 12/20\n",
      "110/110 [==============================] - 30s 270ms/step - loss: 0.0828 - accuracy: 0.9735 - val_loss: 0.5470 - val_accuracy: 0.8657\n",
      "Epoch 13/20\n",
      "110/110 [==============================] - 30s 272ms/step - loss: 0.0739 - accuracy: 0.9738 - val_loss: 0.6874 - val_accuracy: 0.8450\n",
      "Epoch 14/20\n",
      "110/110 [==============================] - 30s 270ms/step - loss: 0.0683 - accuracy: 0.9795 - val_loss: 0.4799 - val_accuracy: 0.8797\n",
      "Epoch 15/20\n",
      "110/110 [==============================] - 29s 265ms/step - loss: 0.0526 - accuracy: 0.9843 - val_loss: 0.6578 - val_accuracy: 0.8720\n",
      "Epoch 16/20\n",
      "110/110 [==============================] - 30s 272ms/step - loss: 0.0624 - accuracy: 0.9833 - val_loss: 0.6459 - val_accuracy: 0.8697\n",
      "Epoch 17/20\n",
      "110/110 [==============================] - 30s 273ms/step - loss: 0.0474 - accuracy: 0.9865 - val_loss: 0.6191 - val_accuracy: 0.8773\n",
      "Epoch 18/20\n",
      "110/110 [==============================] - 30s 275ms/step - loss: 0.0370 - accuracy: 0.9907 - val_loss: 0.7656 - val_accuracy: 0.8707\n",
      "Epoch 19/20\n",
      "110/110 [==============================] - 31s 285ms/step - loss: 0.0500 - accuracy: 0.9875 - val_loss: 0.5966 - val_accuracy: 0.8913\n",
      "Epoch 20/20\n",
      "110/110 [==============================] - 30s 273ms/step - loss: 0.0331 - accuracy: 0.9895 - val_loss: 0.6815 - val_accuracy: 0.8810\n",
      "Time:  0:10:02.030775\n"
     ]
    }
   ],
   "source": [
    "#передадим полученные признаки на вход полносвязному классификатору\n",
    "EPOCHS = 20\n",
    "ACTIVATION='relu'\n",
    "KERNEL_INIT='he_normal'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(vgg_model)\n",
    "vgg_model.trainable = False\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1000, activation=ACTIVATION, input_dim=4*4*512, kernel_initializer=KERNEL_INIT))\n",
    "\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "time_start = datetime.now()\n",
    "history =  model.fit_generator(train_data, steps_per_epoch=len(train_data), shuffle=True, epochs=EPOCHS, validation_steps=len(test_data), validation_data=test_data)\n",
    "time = datetime.now() - time_start\n",
    "print('Time: ', time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
